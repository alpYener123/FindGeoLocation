{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TweetLocationFinder import GatherFiles\n",
    "\n",
    "# First things first, import GatherFiles in order to gather the files needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paths import *\n",
    "# The paths that are used. These are user-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the object\n",
    "files = GatherFiles()\n",
    "\n",
    "# Creates the necessary dictionaries for searching.\n",
    "files.city_parts(path_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aladag': 0, 'ceyhan': 0, 'cukurova': 0, 'feke': 0, 'imamoglu': 0, 'karaisali': 0, 'karatas': 0, 'kozan': 0, 'pozanti': 0, 'saimbeyli': 0, 'saricam': 0, 'seyhan': 0, 'tufanbeyli': 0, 'yumurtalik': 0, 'yuregir': 0}\n",
      "{'aladag': 0, 'madenli': 0, 'buyukmangit': 0, 'ceyhan': 0, 'mustafabeyli': 0, 'huzurevleri': 0, 'mahfesigmaz': 0, 'salbas': 0, 'akkaya': 0, 'feke': 0, 'imamoglu': 0, 'saygecit': 0, 'karaisali': 0, 'karatas': 0, 'hacibeyli': 0, 'kahveli': 0, 'kozan': 0, 'akcatekir': 0, 'gokbez': 0, 'pozanti': 0, 'saimbeyli': 0, 'baklali': 0, 'buruk': 0, 'incirlik': 0, 'osmangazi': 0, 'remzioguzarik': 0, 'akkapi': 0, 'denizli': 0, 'emek': 0, 'fevzipasa': 0, 'gazipasa': 0, 'gulbahcesi': 0, 'gurselpasa': 0, 'hadirli': 0, 'hukumet': 0, 'karayusuflu': 0, 'kurukopru': 0, 'kucukdikili': 0, 'mavi bulvar yurt': 0, 'meydan': 0, 'saydam': 0, 'sakirpasa': 0, 'yagcami': 0, 'yesilevler': 0, 'yesiloba': 0, 'yesilyurt': 0, 'ziyapasa': 0, 'damlali': 0, 'tufanbeyli': 0, 'kalemli': 0, 'yumurtalik': 0, 'abdioglu': 0, 'alihocali': 0, 'bahcelievler': 0, 'dogankent': 0, 'havutlu': 0, 'karacaoglan': 0, 'karsiyaka': 0, 'kazimkarabekir': 0, 'keresteciler': 0, 'pttevleri': 0, 'yakapinar': 0, 'yamacli': 0, 'yavuzlar': 0}\n"
     ]
    }
   ],
   "source": [
    "# Let's check\n",
    "print(files.ilce_dict[\"adana\"])\n",
    "print(files.semt_dict[\"adana\"])\n",
    "\n",
    "# Beware: the 0s in these dictionaries are not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a city list and and an empty data to use. Check README.md for details.\n",
    "\n",
    "city_list = files.city_list_and_data(cities_path=city_list_path, data_path_json=empty_data_path)\n",
    "\n",
    "# Get that empty data into a dictionary\n",
    "city_data = files.get_city_data(empty_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adana', 'adiyaman', 'afyonkarahisar', 'agri', 'aksaray', 'amasya', 'ankara', 'antalya', 'ardahan', 'artvin', 'aydin', 'balikesir', 'bartin', 'batman', 'bayburt', 'bilecik', 'bingol', 'bitlis', 'bolu', 'burdur', 'bursa', 'canakkale', 'cankiri', 'corum', 'denizli', 'diyarbakir', 'duzce', 'edirne', 'elazig', 'erzincan', 'erzurum', 'eskisehir', 'gaziantep', 'giresun', 'gumushane', 'hakkari', 'hatay', 'igdir', 'isparta', 'istanbul', 'izmir', 'kahramanmaras', 'karabuk', 'karaman', 'kars', 'kastamonu', 'kayseri', 'kilis', 'kirikkale', 'kirklareli', 'kirsehir', 'kocaeli', 'konya', 'kutahya', 'malatya', 'manisa', 'mardin', 'mersin', 'mugla', 'mus', 'nevsehir', 'nigde', 'ordu', 'osmaniye', 'rize', 'sakarya', 'samsun', 'sanliurfa', 'siirt', 'sinop', 'sivas', 'sirnak', 'tekirdag', 'tokat', 'trabzon', 'tunceli', 'usak', 'van', 'yalova', 'yozgat', 'zonguldak']\n",
      "{'adana': 0, 'adiyaman': 0, 'afyonkarahisar': 0, 'agri': 0, 'aksaray': 0, 'amasya': 0, 'ankara': 0, 'antalya': 0, 'ardahan': 0, 'artvin': 0, 'aydin': 0, 'balikesir': 0, 'bartin': 0, 'batman': 0, 'bayburt': 0, 'bilecik': 0, 'bingol': 0, 'bitlis': 0, 'bolu': 0, 'burdur': 0, 'bursa': 0, 'canakkale': 0, 'cankiri': 0, 'corum': 0, 'denizli': 0, 'diyarbakir': 0, 'duzce': 0, 'edirne': 0, 'elazig': 0, 'erzincan': 0, 'erzurum': 0, 'eskisehir': 0, 'gaziantep': 0, 'giresun': 0, 'gumushane': 0, 'hakkari': 0, 'hatay': 0, 'igdir': 0, 'isparta': 0, 'istanbul': 0, 'izmir': 0, 'kahramanmaras': 0, 'karabuk': 0, 'karaman': 0, 'kars': 0, 'kastamonu': 0, 'kayseri': 0, 'kilis': 0, 'kirikkale': 0, 'kirklareli': 0, 'kirsehir': 0, 'kocaeli': 0, 'konya': 0, 'kutahya': 0, 'malatya': 0, 'manisa': 0, 'mardin': 0, 'mersin': 0, 'mugla': 0, 'mus': 0, 'nevsehir': 0, 'nigde': 0, 'ordu': 0, 'osmaniye': 0, 'rize': 0, 'sakarya': 0, 'samsun': 0, 'sanliurfa': 0, 'siirt': 0, 'sinop': 0, 'sivas': 0, 'sirnak': 0, 'tekirdag': 0, 'tokat': 0, 'trabzon': 0, 'tunceli': 0, 'usak': 0, 'van': 0, 'yalova': 0, 'yozgat': 0, 'zonguldak': 0}\n"
     ]
    }
   ],
   "source": [
    "# Let's check\n",
    "print(city_list)\n",
    "print(city_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will gather all the data available:\n",
    "# With all 3 methods accumulated on each other (not checking a user if already checked), both with guess and no guess\n",
    "# With all 3 methods individually, with guess and no guess\n",
    "\n",
    "# Let's start with accumulation, with guess\n",
    "from TweetLocationFinder import GatherLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_accumulate = GatherLoc(city_list=city_list, files=files, population_path=pop_path)\n",
    "# Population path example: data/populations.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s, Count=9, Successful Count=3, List1 idx=None, List2 idx=None]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515098it [30:15, 283.72it/s, Count=515098, Successful Count=117421, List1 idx=None, List2 idx=None]\n"
     ]
    }
   ],
   "source": [
    "# Let's start from getting the data of [\"user\"][\"location\"]\n",
    "guess_accumulate.get_user_loc(city_data=city_data, main_data_path=main_data_path, result_path_JSON=accumulation_guess_path_1, user_list_path_TXT=accumulation_guess_path_1_txt, guess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 30mins\n",
    "# Total user count: 515 098\n",
    "# Found user count: 117 421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get back the user id's of the previous move\n",
    "with open(accumulation_guess_path_1_txt, 'r') as file:\n",
    "    user_list = [int(line.strip()) for line in file]\n",
    "\n",
    "# Get the city_data back\n",
    "with open(accumulation_guess_path_1, \"r\") as file:\n",
    "    city_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515098it [29:25, 291.78it/s, Count=515098, Successful Count=29144, List1 idx=117420, List2 idx=None]\n"
     ]
    }
   ],
   "source": [
    "# Now lets get the location data from [\"place\"][\"full_name\"]\n",
    "\n",
    "guess_accumulate.get_tweet_loc(city_data=city_data, main_data_path=main_data_path, result_path_JSON=accumulation_guess_path_2, user_list_path_TXT=accumulation_guess_path_2_txt, guess=True, arg1=user_list)\n",
    "\n",
    "# arg1 will check if the user data has been collected on the previous run. If yes, the user will be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 30mins\n",
    "# Found user count: 29 144\n",
    "# Total users found so far: 146 565"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the city_data back\n",
    "with open(accumulation_guess_path_2, \"r\") as file:\n",
    "    city_data = json.load(file)\n",
    "\n",
    "# Get back the user id's of the previous move\n",
    "with open(accumulation_guess_path_2_txt, 'r') as file:\n",
    "    user_list2 = [int(line.strip()) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515098it [46:33, 184.39it/s, Count=515098, Successful Count=3902, List1 idx=117420, List2 idx=29143]\n"
     ]
    }
   ],
   "source": [
    "# Now lets get the location data from coordinates on [\"geo\"]\n",
    "\n",
    "guess_accumulate.get_tweet_coord(city_data=city_data, main_data_path=main_data_path, turkey_geoJSON_path=turkey_geoJSON_path, result_path_JSON=accumulation_guess_path_3, user_list_path_TXT=accumulation_guess_path_3_txt, arg1=user_list, arg2=user_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 50 mins\n",
    "# Found user count: 3 902\n",
    "# Total users: 150 467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For individual data, copy the [\"user\"][\"location\"] data gathered since it was the first step in the cumulation so it was individual as well\n",
    "\n",
    "import json\n",
    "\n",
    "with open(accumulation_guess_path_1, \"r\") as file:\n",
    "    user_loc_data = json.load(file)\n",
    "\n",
    "\n",
    "with open(individual_guess_path_1, 'w') as file:\n",
    "            json.dump(user_loc_data, file, indent=4)\n",
    "\n",
    "# Get an empty city data\n",
    "\n",
    "city_data = files.get_city_data(empty_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515098it [29:36, 289.96it/s, Count=515098, Successful Count=48172, List1 idx=None, List2 idx=None]\n"
     ]
    }
   ],
   "source": [
    "# Now get the [\"place\"][\"full_name\"] individually, without the kwargs\n",
    "\n",
    "# Since we are not accumulating, the user id lists are not important hence, \n",
    "# the argument \"user_list_path_TXT\" is not needed.\n",
    "\n",
    "guess_accumulate.get_tweet_loc(city_data=city_data, main_data_path=main_data_path, result_path_JSON=individual_guess_path_2, guess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 30 mins\n",
    "# Users found: 48 172\n",
    "\n",
    "# Now do the same for [\"geo\"] coordinates as well (coordinates does not have a guessing option)\n",
    "\n",
    "city_data = files.get_city_data(empty_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4863it [01:22, 58.94it/s, Count=4863, Successful Count=206, List1 idx=None, List2 idx=None] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alpye\\OneDrive\\Desktop\\pure\\FindGeoLocation\\LocationProject\\trial_data.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alpye/OneDrive/Desktop/pure/FindGeoLocation/LocationProject/trial_data.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m guess_accumulate\u001b[39m.\u001b[39;49mget_tweet_coord(city_data\u001b[39m=\u001b[39;49mcity_data, main_data_path\u001b[39m=\u001b[39;49mmain_data_path, turkey_geoJSON_path\u001b[39m=\u001b[39;49mturkey_geoJSON_path, result_path_JSON\u001b[39m=\u001b[39;49mindividual_coordinates_path)\n",
      "File \u001b[1;32mc:\\Users\\alpye\\OneDrive\\Desktop\\pure\\FindGeoLocation\\LocationProject\\TweetLocationFinder\\GatherUserLoc\\GatherLoc.py:442\u001b[0m, in \u001b[0;36mGatherLoc.get_tweet_coord\u001b[1;34m(self, city_data, main_data_path, turkey_geoJSON_path, result_path_JSON, user_list_path_TXT, **kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m pt \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39mGeoDataFrame({\u001b[39m'\u001b[39m\u001b[39mgeometry\u001b[39m\u001b[39m'\u001b[39m: [geometry]})\n\u001b[0;32m    440\u001b[0m pt \u001b[39m=\u001b[39m pt\u001b[39m.\u001b[39mset_crs(\u001b[39m'\u001b[39m\u001b[39mepsg:4326\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 442\u001b[0m intersections \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39;49moverlay(pt, df, how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mintersection\u001b[39;49m\u001b[39m'\u001b[39;49m, keep_geom_type\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    443\u001b[0m name \u001b[39m=\u001b[39m intersections[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mempty \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\coding\\PITON_COMPILER\\Lib\\site-packages\\geopandas\\tools\\overlay.py:310\u001b[0m, in \u001b[0;36moverlay\u001b[1;34m(df1, df2, how, keep_geom_type, make_valid)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n\u001b[0;32m    309\u001b[0m df1 \u001b[39m=\u001b[39m _make_valid(df1)\n\u001b[1;32m--> 310\u001b[0m df2 \u001b[39m=\u001b[39m _make_valid(df2)\n\u001b[0;32m    312\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():  \u001b[39m# CRS checked above, suppress array-level warning\u001b[39;00m\n\u001b[0;32m    313\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCRS mismatch between the CRS\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\coding\\PITON_COMPILER\\Lib\\site-packages\\geopandas\\tools\\overlay.py:296\u001b[0m, in \u001b[0;36moverlay.<locals>._make_valid\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    294\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    295\u001b[0m \u001b[39mif\u001b[39;00m df\u001b[39m.\u001b[39mgeom_type\u001b[39m.\u001b[39misin(polys)\u001b[39m.\u001b[39mall():\n\u001b[1;32m--> 296\u001b[0m     mask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mdf\u001b[39m.\u001b[39;49mgeometry\u001b[39m.\u001b[39;49mis_valid\n\u001b[0;32m    297\u001b[0m     col \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39m_geometry_column_name\n\u001b[0;32m    298\u001b[0m     \u001b[39mif\u001b[39;00m make_valid:\n",
      "File \u001b[1;32mc:\\coding\\PITON_COMPILER\\Lib\\site-packages\\geopandas\\base.py:293\u001b[0m, in \u001b[0;36mGeoPandasBase.is_valid\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_valid\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39m    geometries that are valid.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \n\u001b[0;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mreturn\u001b[39;00m _delegate_property(\u001b[39m\"\u001b[39;49m\u001b[39mis_valid\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\coding\\PITON_COMPILER\\Lib\\site-packages\\geopandas\\base.py:66\u001b[0m, in \u001b[0;36m_delegate_property\u001b[1;34m(op, this)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_delegate_property\u001b[39m(op, this):\n\u001b[0;32m     64\u001b[0m     \u001b[39m# type: (str, GeoSeries) -> GeoSeries/Series\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     a_this \u001b[39m=\u001b[39m GeometryArray(this\u001b[39m.\u001b[39mgeometry\u001b[39m.\u001b[39mvalues)\n\u001b[1;32m---> 66\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(a_this, op)\n\u001b[0;32m     67\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, GeometryArray):\n\u001b[0;32m     68\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgeoseries\u001b[39;00m \u001b[39mimport\u001b[39;00m GeoSeries\n",
      "File \u001b[1;32mc:\\coding\\PITON_COMPILER\\Lib\\site-packages\\geopandas\\array.py:467\u001b[0m, in \u001b[0;36mGeometryArray.is_valid\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    466\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_valid\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 467\u001b[0m     \u001b[39mreturn\u001b[39;00m vectorized\u001b[39m.\u001b[39;49mis_valid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data)\n",
      "File \u001b[1;32mc:\\coding\\PITON_COMPILER\\Lib\\site-packages\\geopandas\\_vectorized.py:484\u001b[0m, in \u001b[0;36mis_valid\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_valid\u001b[39m(data):\n\u001b[0;32m    483\u001b[0m     \u001b[39mif\u001b[39;00m compat\u001b[39m.\u001b[39mUSE_SHAPELY_20:\n\u001b[1;32m--> 484\u001b[0m         \u001b[39mreturn\u001b[39;00m shapely\u001b[39m.\u001b[39;49mis_valid(data)\n\u001b[0;32m    485\u001b[0m     \u001b[39melif\u001b[39;00m compat\u001b[39m.\u001b[39mUSE_PYGEOS:\n\u001b[0;32m    486\u001b[0m         \u001b[39mreturn\u001b[39;00m pygeos\u001b[39m.\u001b[39mis_valid(data)\n",
      "File \u001b[1;32mc:\\coding\\PITON_COMPILER\\Lib\\site-packages\\shapely\\decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m array_args:\n\u001b[0;32m     76\u001b[0m         arr\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     78\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[39mfor\u001b[39;00m arr, old_flag \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[1;32mc:\\coding\\PITON_COMPILER\\Lib\\site-packages\\shapely\\predicates.py:402\u001b[0m, in \u001b[0;36mis_valid\u001b[1;34m(geometry, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[0;32m    401\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 402\u001b[0m     result \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mis_valid(geometry, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    403\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "guess_accumulate.get_tweet_coord(city_data=city_data, main_data_path=main_data_path, turkey_geoJSON_path=turkey_geoJSON_path, result_path_JSON=individual_coordinates_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 2hrs 30 mins\n",
    "# Users found: 22 050\n",
    "\n",
    "# Now, we do all this for the parameter guess=False\n",
    "city_data = files.get_city_data(empty_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515098it [30:44, 279.31it/s, Count=515098, Successful Count=116893, List1 idx=None, List2 idx=None]\n"
     ]
    }
   ],
   "source": [
    "# Let's start from getting the data of [\"user\"][\"location\"]\n",
    "\n",
    "guess_accumulate.get_user_loc(city_data=city_data, main_data_path=main_data_path, result_path_JSON=accumulation_noguess_path_1, user_list_path_TXT=accumulation_noguess_path_1_txt, guess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 30 mins\n",
    "# Users found: 116 893"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the city_data back\n",
    "with open(accumulation_noguess_path_1, \"r\") as file:\n",
    "    city_data = json.load(file)\n",
    "\n",
    "# Get back the user id's of the previous move\n",
    "with open(accumulation_noguess_path_1_txt, 'r') as file:\n",
    "    user_list1 = [int(line.strip()) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515098it [29:04, 295.20it/s, Count=515098, Successful Count=29129, List1 idx=116892, List2 idx=None]\n"
     ]
    }
   ],
   "source": [
    "# Now do for [\"place\"][\"full_name\"]\n",
    "\n",
    "guess_accumulate.get_tweet_loc(city_data=city_data, main_data_path=main_data_path, result_path_JSON=accumulation_noguess_path_2, user_list_path_TXT=accumulation_noguess_path_2_txt, guess=False ,arg1=user_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 30 mins\n",
    "# Users found: 29 129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the city_data back\n",
    "with open(accumulation_noguess_path_2, \"r\") as file:\n",
    "    city_data = json.load(file)\n",
    "\n",
    "# Get back the user id's of the previous move\n",
    "with open(accumulation_noguess_path_2_txt, 'r') as file:\n",
    "    user_list2 = [int(line.strip()) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515098it [46:36, 184.18it/s, Count=515098, Successful Count=3955, List1 idx=116892, List2 idx=29128]\n"
     ]
    }
   ],
   "source": [
    "# Finally, do for [\"geo\"] coordinates\n",
    "guess_accumulate.get_tweet_coord(city_data=city_data, main_data_path=main_data_path, turkey_geoJSON_path=turkey_geoJSON_path, result_path_JSON=accumulation_noguess_path_3, user_list_path_TXT=accumulation_noguess_path_3_txt, arg1=user_list1, arg2=user_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 45 mins\n",
    "# Users found: 3 955\n",
    "\n",
    "# Now let's do it for the individual part, for guess=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For individual data of no guess, copy [\"user\"][\"location\"] data since it was the first step in the cumulation so it was individual as well\n",
    "\n",
    "import json\n",
    "\n",
    "with open(accumulation_noguess_path_1, \"r\") as file:\n",
    "    user_loc_data = json.load(file)\n",
    "\n",
    "# Write it in its designated path\n",
    "with open(individual_noguess_path_1, 'w') as file:\n",
    "            json.dump(user_loc_data, file, indent=4)\n",
    "\n",
    "# Get a fresh new city_data\n",
    "\n",
    "city_data = files.get_city_data(empty_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s, Count=2, Successful Count=0, List1 idx=None, List2 idx=None]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "515098it [39:40, 216.39it/s, Count=515098, Successful Count=47993, List1 idx=None, List2 idx=None]\n"
     ]
    }
   ],
   "source": [
    "# Get the [\"place\"][\"full_name\"] data individually\n",
    "guess_accumulate.get_tweet_loc(city_data=city_data, main_data_path=main_data_path, result_path_JSON=individual_noguess_path_2, guess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 40 mins\n",
    "# Users found: 47 993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/alpye/OneDrive/Desktop/pure/FindGeoLocation/trial_data_gathered/individual/guess/coordinates.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alpye\\OneDrive\\Desktop\\pure\\FindGeoLocation\\LocationProject\\trial_data.ipynb Cell 33\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alpye/OneDrive/Desktop/pure/FindGeoLocation/LocationProject/trial_data.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# For [\"geo\"] coordinates, it is the same as the guess individual one since it does not have a guess argument\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alpye/OneDrive/Desktop/pure/FindGeoLocation/LocationProject/trial_data.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alpye/OneDrive/Desktop/pure/FindGeoLocation/LocationProject/trial_data.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(individual_coordinates_path, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alpye/OneDrive/Desktop/pure/FindGeoLocation/LocationProject/trial_data.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     user_loc_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alpye/OneDrive/Desktop/pure/FindGeoLocation/LocationProject/trial_data.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(individual_coordinates_path_noguess, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/alpye/OneDrive/Desktop/pure/FindGeoLocation/trial_data_gathered/individual/guess/coordinates.json'"
     ]
    }
   ],
   "source": [
    "# For [\"geo\"] coordinates, it is the same as the guess individual one since it does not have a guess argument\n",
    "\n",
    "import json\n",
    "\n",
    "with open(individual_coordinates_path, \"r\") as file:\n",
    "    user_loc_data = json.load(file)\n",
    "\n",
    "with open(individual_coordinates_path_noguess, 'w') as file:\n",
    "            json.dump(user_loc_data, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
